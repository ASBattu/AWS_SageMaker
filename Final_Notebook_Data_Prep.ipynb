{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries to be used\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import copy\n",
    "import re\n",
    "import string\n",
    "\n",
    "# To remove stopwords from text\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words(\"english\"))\n",
    "\n",
    "# To finetune BERT model on custom dataset\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import .txt files to create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute path to parse text files in the folder\n",
    "text_path = r'D:\\Coding\\Twitter_Sentimental_Analysis\\Training_Data/*.txt'\n",
    "text_files = glob.glob(text_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe from Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary and loop all text files as dataframes into it\n",
    "text_dictionary = {}\n",
    "\n",
    "for index, file in enumerate(text_files):\n",
    "    temp_df = pd.read_csv(text_files[index], delimiter = \"\\t\", header = None)\n",
    "    text_dictionary[index] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amito\\AppData\\Local\\Temp\\ipykernel_11376\\1772446728.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_dataframe = data_dataframe.append(temp_df, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# Create the dataframe containing text to classify and its sentiment\n",
    "data_dataframe = pd.DataFrame(columns=['Sentiment', 'Text'])\n",
    "\n",
    "# Loop through all dataframes, and only take the columns with sentiment and text\n",
    "# Sentiment word list\n",
    "sentiment_word_list = ['neutral', 'negative', '-1', 'positive', '0', '1']\n",
    "\n",
    "for current_df in range(len(text_dictionary)):\n",
    "    for index, value in enumerate(text_dictionary[current_df].iloc[0,:]):\n",
    "        if str(value) in sentiment_word_list:\n",
    "\n",
    "            # Create temporary Dataframe with the two columns (sentiment and text)\n",
    "            temp_df = pd.DataFrame({'Sentiment': text_dictionary[current_df].iloc[:,index], 'Text': text_dictionary[current_df].iloc[:,index+1]})\n",
    "\n",
    "            # Append the temporary DataFrame to the original Dataframe\n",
    "            data_dataframe = data_dataframe.append(temp_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary for sentiment values\n",
    "sentiment_mapping = {\n",
    "    'positive': 1,\n",
    "    'negative': -1,\n",
    "    2: 1,\n",
    "    -2: -1,\n",
    "}\n",
    "\n",
    "# Convert sentiment values using the mapping dictionary\n",
    "data_dataframe = data_dataframe.replace({\"Sentiment\": sentiment_mapping})\n",
    "\n",
    "# Drop rows with neutral or off-topic sentiment\n",
    "data_dataframe = data_dataframe[~data_dataframe['Sentiment'].isin(['neutral', 0, 'off topic'])]\n",
    "\n",
    "# Sort dataframe via the 'Sentiment column\n",
    "data_dataframe = data_dataframe.sort_values(by=['Sentiment']).reset_index().drop(columns = ['index'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance dataset before pre-processing from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    71986\n",
       "-1    23420\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current balance of sentiments\n",
    "data_dataframe['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    23420\n",
       " 1    23420\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only copy equal datapoints\n",
    "balanced_data_dataframe = copy.deepcopy(data_dataframe[0:23420*2])\n",
    "balanced_data_dataframe['Sentiment'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text before saving into .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text and remove punctuation, stopwords and whitespace\n",
    "def preprocess_text(text):\n",
    "    # Remove http / https links\n",
    "    text = re.sub(r'http\\S+|https\\S+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stoplist)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Convert -1 to 0 ---> for binary classification\n",
    "def preprocess_sentiment(text):\n",
    "    if text == -1:\n",
    "        text = 0\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pre-processing techniques to both the columns\n",
    "balanced_data_dataframe['Text'] = balanced_data_dataframe['Text'].apply(preprocess_text)\n",
    "balanced_data_dataframe['Sentiment'] = balanced_data_dataframe['Sentiment'].apply(preprocess_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the DataFrame as a CSV file for viewing\n",
    "# balanced_data_dataframe.to_csv('balanced_train_df.csv', sep=' ', index=False)\n",
    "\n",
    "# # Load saved CSV into DataFrame\n",
    "# balanced_data_dataframe = pd.read_csv('balanced_train_df.csv', sep = ' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
